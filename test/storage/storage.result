test_run = require('test_run').new()
---
...
netbox = require('net.box')
---
...
REPLICASET_1 = { 'storage_1_a', 'storage_1_b' }
---
...
REPLICASET_2 = { 'storage_2_a', 'storage_2_b' }
---
...
engine = test_run:get_cfg('engine')
---
...
test_run:create_cluster(REPLICASET_1, 'storage')
---
...
test_run:create_cluster(REPLICASET_2, 'storage')
---
...
util = require('util')
---
...
util.wait_master(test_run, REPLICASET_1, 'storage_1_a')
---
...
util.wait_master(test_run, REPLICASET_2, 'storage_2_a')
---
...
util.map_evals(test_run, {REPLICASET_1, REPLICASET_2}, 'bootstrap_storage(\'%s\')', engine)
---
...
util.push_rs_filters(test_run)
---
...
_ = test_run:switch("storage_2_a")
---
...
vshard.storage.rebalancer_disable()
---
...
_ = test_run:switch("storage_1_a")
---
...
-- Ensure the trigger is called right after setting, and does not
-- wait until reconfiguration.
master_enabled = false
---
...
master_disabled = false
---
...
function on_master_enable() master_enabled = true end
---
...
function on_master_disable() master_disabled = true end
---
...
vshard.storage.on_master_enable(on_master_enable)
---
...
vshard.storage.on_master_disable(on_master_disable)
---
...
master_enabled
---
- true
...
master_disabled
---
- false
...
-- Test the same about master disabling.
_ = test_run:switch('storage_1_b')
---
...
master_enabled = false
---
...
master_disabled = false
---
...
function on_master_enable() master_enabled = true end
---
...
function on_master_disable() master_disabled = true end
---
...
vshard.storage.on_master_enable(on_master_enable)
---
...
vshard.storage.on_master_disable(on_master_disable)
---
...
master_enabled
---
- false
...
master_disabled
---
- true
...
_ = test_run:switch('storage_1_a')
---
...
vshard.storage.info().replicasets[util.replicasets[1]] or vshard.storage.info()
---
- uuid: <replicaset_1>
  master:
    uri: storage@127.0.0.1:3301
...
vshard.storage.info().replicasets[util.replicasets[2]] or vshard.storage.info()
---
- uuid: <replicaset_2>
  master:
    uri: storage@127.0.0.1:3303
...
-- Try to call info on a replicaset with no master.
rs1 = vshard.storage.internal.replicasets[util.replicasets[1]]
---
...
saved_master = rs1.master
---
...
rs1.master = nil
---
...
vshard.storage.info()
---
- replicasets:
    <replicaset_2>:
      uuid: <replicaset_2>
      master:
        uri: storage@127.0.0.1:3303
    <replicaset_1>:
      uuid: <replicaset_1>
      master: missing
  bucket:
    receiving: 0
    active: 0
    total: 0
    garbage: 0
    pinned: 0
    sending: 0
  status: 2
  replication:
    status: slave
  alerts:
  - ['MISSING_MASTER', 'Master is not configured for replicaset <replicaset_1>']
...
rs1.master = saved_master
---
...
-- Sync API
vshard.storage.sync()
---
- true
...
util.check_error(vshard.storage.sync, "xxx")
---
- 'Usage: vshard.storage.sync([timeout: number])'
...
vshard.storage.sync(100500)
---
- true
...
vshard.storage.buckets_info()
---
- {}
...
vshard.storage.bucket_force_create(1)
---
- true
...
vshard.storage.buckets_info()
---
- 1:
    status: active
    id: 1
...
vshard.storage.bucket_force_create(1) -- error
---
- null
- Duplicate key exists in unique index 'pk' in space '_bucket'
...
vshard.storage.bucket_force_drop(1)
---
- true
...
vshard.storage.buckets_info()
---
- {}
...
vshard.storage.bucket_force_create(1)
---
- true
...
vshard.storage.bucket_force_create(2)
---
- true
...
_ = test_run:switch("storage_2_a")
---
...
vshard.storage.bucket_force_create(3)
---
- true
...
vshard.storage.bucket_force_create(4)
---
- true
...
_ = test_run:switch("storage_2_b")
---
...
box.cfg{replication_timeout = 0.01}
---
...
vshard.storage.info()
---
- replicasets:
    <replicaset_2>:
      uuid: <replicaset_2>
      master:
        uri: storage@127.0.0.1:3303
    <replicaset_1>:
      uuid: <replicaset_1>
      master:
        uri: storage@127.0.0.1:3301
  bucket:
    receiving: 0
    active: 2
    total: 2
    garbage: 0
    pinned: 0
    sending: 0
  status: 0
  replication:
    status: follow
    lag: <lag>
  alerts: []
...
_ = test_run:cmd("stop server storage_2_a")
---
...
box.cfg{replication_timeout = 0.01}
---
...
vshard.storage.info()
---
- replicasets:
    <replicaset_2>:
      uuid: <replicaset_2>
      master:
        uri: storage@127.0.0.1:3303
    <replicaset_1>:
      uuid: <replicaset_1>
      master:
        uri: storage@127.0.0.1:3301
  bucket:
    receiving: 0
    active: 2
    total: 2
    garbage: 0
    pinned: 0
    sending: 0
  status: 1
  replication:
    status: disconnected
    idle: <idle>
  alerts:
  - ['UNREACHABLE_MASTER', 'Master of replicaset <replicaset_2>
      is unreachable: disconnected']
...
_ = test_run:cmd("start server storage_2_a")
---
...
_ = test_run:switch("storage_2_a")
---
...
fiber = require('fiber')
---
...
info = vshard.storage.info()
---
...
--
-- gh-144: do not warn about low redundancy if it is a desired
-- case.
--
while #info.alerts ~= 0 do fiber.sleep(0.1) info = vshard.storage.info() end
---
...
info
---
- replicasets:
    <replicaset_2>:
      uuid: <replicaset_2>
      master:
        state: active
        uri: storage@127.0.0.1:3303
        uuid: <storage_2_a>
    <replicaset_1>:
      uuid: <replicaset_1>
      master:
        state: active
        uri: storage@127.0.0.1:3301
        uuid: <storage_1_a>
  bucket:
    receiving: 0
    active: 2
    total: 2
    garbage: 0
    pinned: 0
    sending: 0
  status: 0
  replication:
    status: master
  alerts: []
...
_ = test_run:cmd("stop server storage_2_b")
---
...
vshard.storage.info()
---
- replicasets:
    <replicaset_2>:
      uuid: <replicaset_2>
      master:
        state: active
        uri: storage@127.0.0.1:3303
        uuid: <storage_2_a>
    <replicaset_1>:
      uuid: <replicaset_1>
      master:
        state: active
        uri: storage@127.0.0.1:3301
        uuid: <storage_1_a>
  bucket:
    receiving: 0
    active: 2
    total: 2
    garbage: 0
    pinned: 0
    sending: 0
  status: 3
  replication:
    status: master
  alerts:
  - ['UNREACHABLE_REPLICA', 'Replica <storage_2_b> isn''t active']
  - ['UNREACHABLE_REPLICASET', 'There is no active replicas in replicaset <replicaset_2>']
...
_ = test_run:cmd("start server storage_2_b")
---
...
_ = test_run:switch("storage_2_b")
---
...
vshard.storage.info()
---
- replicasets:
    <replicaset_2>:
      uuid: <replicaset_2>
      master:
        uri: storage@127.0.0.1:3303
    <replicaset_1>:
      uuid: <replicaset_1>
      master:
        uri: storage@127.0.0.1:3301
  bucket:
    receiving: 0
    active: 2
    total: 2
    garbage: 0
    pinned: 0
    sending: 0
  status: 0
  replication:
    status: follow
    lag: <lag>
  alerts: []
...
_ = test_run:switch("storage_2_a")
---
...
vshard.storage.info()
---
- replicasets:
    <replicaset_2>:
      uuid: <replicaset_2>
      master:
        state: active
        uri: storage@127.0.0.1:3303
        uuid: <storage_2_a>
    <replicaset_1>:
      uuid: <replicaset_1>
      master:
        state: active
        uri: storage@127.0.0.1:3301
        uuid: <storage_1_a>
  bucket:
    receiving: 0
    active: 2
    total: 2
    garbage: 0
    pinned: 0
    sending: 0
  status: 0
  replication:
    status: master
  alerts: []
...
_ = test_run:switch("storage_1_a")
---
...
_ = test_run:cmd("setopt delimiter ';'")
---
...
box.begin()
for id = 1, 8 do
    local bucket_id = id % 4
    box.space.test:insert({id, bucket_id})
    for id2 = id * 10, id * 10 + 2 do
        box.space.test2:insert({id2, id, bucket_id})
    end
end
box.commit();
---
...
_ = test_run:cmd("setopt delimiter ''");
---
...
box.space.test:select()
---
- - [1, 1]
  - [2, 2]
  - [3, 3]
  - [4, 0]
  - [5, 1]
  - [6, 2]
  - [7, 3]
  - [8, 0]
...
box.space.test2:select()
---
- - [10, 1, 1]
  - [11, 1, 1]
  - [12, 1, 1]
  - [20, 2, 2]
  - [21, 2, 2]
  - [22, 2, 2]
  - [30, 3, 3]
  - [31, 3, 3]
  - [32, 3, 3]
  - [40, 4, 0]
  - [41, 4, 0]
  - [42, 4, 0]
  - [50, 5, 1]
  - [51, 5, 1]
  - [52, 5, 1]
  - [60, 6, 2]
  - [61, 6, 2]
  - [62, 6, 2]
  - [70, 7, 3]
  - [71, 7, 3]
  - [72, 7, 3]
  - [80, 8, 0]
  - [81, 8, 0]
  - [82, 8, 0]
...
vshard.storage.bucket_collect(1)
---
- - - 514
    - - [10, 1, 1]
      - [11, 1, 1]
      - [12, 1, 1]
  - - 513
    - - [1, 1]
      - [5, 1]
...
vshard.storage.bucket_collect(2)
---
- - - 514
    - - [20, 2, 2]
      - [21, 2, 2]
      - [22, 2, 2]
  - - 513
    - - [2, 2]
      - [6, 2]
...
box.space.test:get{1}
---
- [1, 1]
...
vshard.storage.call(1, 'read', 'space_get', {'test', {1}})
---
- true
- [1, 1]
- null
- null
...
vshard.storage.call(100500, 'read', 'space_get', {'test', {1}})
---
- null
- bucket_id: 100500
  reason: Not found
  code: 1
  type: ShardingError
  message: 'Cannot perform action with bucket 100500, reason: Not found'
  name: WRONG_BUCKET
...
--
-- Test not existing uuid.
--
vshard.storage.bucket_recv(100, 'from_uuid', {{1000, {{1}}}})
---
- null
- code: 4
  type: ShardingError
  name: NO_SUCH_REPLICASET
  replicaset_uuid: from_uuid
  message: Replicaset from_uuid not found
...
--
-- Test not existing space in bucket data.
--
vshard.storage.bucket_recv(4, util.replicasets[2], {{1000, {{1}}}})
---
- null
- type: ClientError
  code: 36
  message: Space '1000' does not exist
  trace:
  - file: '[C]'
    line: 4294967295
...
while box.space._bucket:get{4} do vshard.storage.recovery_wakeup() fiber.sleep(0.01) end
---
...
--
-- Bucket transfer
--
-- Transfer to unknown replicaset.
vshard.storage.bucket_send(1, 'unknown uuid')
---
- null
- code: 4
  type: ShardingError
  name: NO_SUCH_REPLICASET
  replicaset_uuid: unknown uuid
  message: Replicaset unknown uuid not found
...
-- Successful transfer.
vshard.storage.bucket_send(1, util.replicasets[2])
---
- true
...
_ = test_run:switch("storage_2_a")
---
...
vshard.storage.buckets_info()
---
- 1:
    status: active
    id: 1
  3:
    status: active
    id: 3
  4:
    status: active
    id: 4
...
_ = test_run:switch("storage_1_a")
---
...
vshard.storage.buckets_info()
---
- 1:
    status: sent
    ro_lock: true
    destination: <replicaset_2>
    id: 1
  2:
    status: active
    id: 2
...
--
-- Part of gh-76: check that netbox old connections are reused on
-- reconfiguration.
--
old_connections = {}
---
...
connection_count = 0
---
...
old_replicasets = vshard.storage.internal.replicasets
---
...
_ = test_run:cmd("setopt delimiter ';'")
---
...
for _, old_replicaset in pairs(old_replicasets) do
	for uuid, old_replica in pairs(old_replicaset.replicas) do
		old_connections[uuid] = old_replica.conn
		if old_replica.conn then
			connection_count = connection_count + 1
		end
	end
end;
---
...
_ = test_run:cmd("setopt delimiter ''");
---
...
connection_count > 0
---
- true
...
vshard.storage.cfg(cfg, util.name_to_uuid.storage_1_a)
---
...
new_replicasets = vshard.storage.internal.replicasets
---
...
new_replicasets ~= old_replicasets
---
- true
...
_ = test_run:cmd("setopt delimiter ';'")
---
...
for _, new_replicaset in pairs(new_replicasets) do
	for uuid, new_replica in pairs(new_replicaset.replicas) do
		assert(old_connections[uuid] == new_replica.conn)
	end
end;
---
...
_ = test_run:cmd("setopt delimiter ''");
---
...
-- gh-114: Check non-dynamic option change during reconfigure.
non_dynamic_cfg = table.copy(cfg)
---
...
non_dynamic_cfg.bucket_count = require('vshard.consts').DEFAULT_BUCKET_COUNT + 1
---
...
util.check_error(vshard.storage.cfg, non_dynamic_cfg, util.name_to_uuid.storage_1_a)
---
- Non-dynamic option bucket_count cannot be reconfigured
...
-- Error during reconfigure process.
_, rs = next(vshard.storage.internal.replicasets)
---
...
rs:callro('echo', {'some_data'})
---
- some_data
- null
- null
...
vshard.storage.internal.errinj.ERRINJ_CFG = true
---
...
old_internal = table.copy(vshard.storage.internal)
---
...
util.check_error(vshard.storage.cfg, cfg, util.name_to_uuid.storage_1_a)
---
- 'Error injection: cfg'
...
vshard.storage.internal.errinj.ERRINJ_CFG = false
---
...
util.has_same_fields(old_internal, vshard.storage.internal)
---
- true
...
_, rs = next(vshard.storage.internal.replicasets)
---
...
rs:callro('echo', {'some_data'})
---
- some_data
- null
- null
...
_ = test_run:switch("default")
---
...
test_run:drop_cluster(REPLICASET_2)
---
...
test_run:drop_cluster(REPLICASET_1)
---
...
_ = test_run:cmd('clear filter')
---
...
