test_run = require('test_run').new()
---
...
netbox = require('net.box')
---
...
REPLICASET_1 = { 'storage_1_a', 'storage_1_b' }
---
...
REPLICASET_2 = { 'storage_2_a', 'storage_2_b' }
---
...
engine = test_run:get_cfg('engine')
---
...
test_run:create_cluster(REPLICASET_1, 'storage')
---
...
test_run:create_cluster(REPLICASET_2, 'storage')
---
...
util = require('util')
---
...
util.wait_master(test_run, REPLICASET_1, 'storage_1_a')
---
...
util.wait_master(test_run, REPLICASET_2, 'storage_2_a')
---
...
util.map_evals(test_run, {REPLICASET_1, REPLICASET_2}, 'bootstrap_storage(\'%s\')', engine)
---
...
util.push_rs_filters(test_run)
---
...
_ = test_run:switch("storage_2_a")
---
...
vshard.storage.rebalancer_disable()
---
...
_ = test_run:switch("storage_1_a")
---
...
-- Ensure the trigger is called right after setting, and does not
-- wait until reconfiguration.
master_enabled = false
---
...
master_disabled = false
---
...
function on_master_enable() master_enabled = true end
---
...
function on_master_disable() master_disabled = true end
---
...
_ = vshard.storage.on_master_enable(on_master_enable)
---
...
_ = vshard.storage.on_master_disable(on_master_disable)
---
...
master_enabled
---
- true
...
master_disabled
---
- false
...
-- Test the same about master disabling.
_ = test_run:switch('storage_1_b')
---
...
master_enabled = false
---
...
master_disabled = false
---
...
function on_master_enable() master_enabled = true end
---
...
function on_master_disable() master_disabled = true end
---
...
_ = vshard.storage.on_master_enable(on_master_enable)
---
...
_ = vshard.storage.on_master_disable(on_master_disable)
---
...
master_enabled
---
- false
...
master_disabled
---
- true
...
_ = test_run:switch('storage_1_a')
---
...
vshard.storage.info().replicasets[util.replicasets[1]] or vshard.storage.info()
---
- uuid: <replicaset_1>
  master:
    uri: storage@127.0.0.1:3301
...
vshard.storage.info().replicasets[util.replicasets[2]] or vshard.storage.info()
---
- uuid: <replicaset_2>
  master:
    uri: storage@127.0.0.1:3303
...
-- Try to call info on a replicaset with no master.
rs1 = vshard.storage.internal.replicasets[util.replicasets[1]]
---
...
saved_master = rs1.master
---
...
rs1.master = nil
---
...
assert(vshard.storage.internal.is_master)
---
- true
...
vshard.storage.internal.is_master = false
---
...
vshard.storage.info()
---
- replicasets:
    <replicaset_2>:
      uuid: <replicaset_2>
      master:
        uri: storage@127.0.0.1:3303
    <replicaset_1>:
      uuid: <replicaset_1>
      master: missing
  bucket:
    receiving: 0
    active: 0
    total: 0
    garbage: 0
    pinned: 0
    sending: 0
  status: 2
  replication:
    status: slave
  alerts:
  - ['MISSING_MASTER', 'Master is not configured for replicaset <replicaset_1>']
...
vshard.storage.internal.is_master = true
---
...
rs1.master = saved_master
---
...
-- Sync API
vshard.storage.sync()
---
- true
...
util.check_error(vshard.storage.sync, "xxx")
---
- 'Usage: vshard.storage.sync([timeout: number])'
...
vshard.storage.sync(100500)
---
- true
...
vshard.storage.buckets_info()
---
- {}
...
vshard.storage.bucket_force_create(1)
---
- true
...
vshard.storage.buckets_info()
---
- 1:
    status: active
    id: 1
...
ok, err = vshard.storage.bucket_force_create(1)
---
...
assert(not ok and err.message:match("Duplicate key exists"))
---
- Duplicate key exists
...
_ = test_run:switch('default')
---
...
util.map_bucket_protection(test_run, {REPLICASET_1}, false)
---
...
test_run:switch('storage_1_a')
---
- true
...
vshard.storage.bucket_force_drop(1)
---
- true
...
vshard.storage.sync()
---
- true
...
_ = test_run:switch('default')
---
...
util.map_bucket_protection(test_run, {REPLICASET_1}, true)
---
...
test_run:switch('storage_1_a')
---
- true
...
vshard.storage.buckets_info()
---
- {}
...
vshard.storage.bucket_force_create(1)
---
- true
...
vshard.storage.bucket_force_create(2)
---
- true
...
_ = test_run:switch("storage_2_a")
---
...
vshard.storage.bucket_force_create(3)
---
- true
...
vshard.storage.bucket_force_create(4)
---
- true
...
_ = test_run:switch("storage_2_b")
---
...
box.cfg{replication_timeout = 0.01}
---
...
vshard.storage.info()
---
- replicasets:
    <replicaset_2>:
      uuid: <replicaset_2>
      master:
        uri: storage@127.0.0.1:3303
    <replicaset_1>:
      uuid: <replicaset_1>
      master:
        uri: storage@127.0.0.1:3301
  bucket:
    receiving: 0
    active: 2
    total: 2
    garbage: 0
    pinned: 0
    sending: 0
  status: 0
  replication:
    status: follow
    lag: <lag>
  alerts: []
...
_ = test_run:cmd("stop server storage_2_a")
---
...
box.cfg{replication_timeout = 0.01}
---
...
alerts = vshard.storage.info().alerts
---
...
#alerts == 1 and alerts[1][1] or alerts
---
- UNREACHABLE_MASTER
...
_ = test_run:cmd("start server storage_2_a")
---
...
_ = test_run:switch("storage_2_a")
---
...
fiber = require('fiber')
---
...
info = vshard.storage.info()
---
...
--
-- gh-144: do not warn about low redundancy if it is a desired
-- case.
--
while #info.alerts ~= 0 do fiber.sleep(0.1) info = vshard.storage.info() end
---
...
info
---
- replicasets:
    <replicaset_2>:
      uuid: <replicaset_2>
      master:
        state: active
        uri: storage@127.0.0.1:3303
        uuid: <storage_2_a>
    <replicaset_1>:
      uuid: <replicaset_1>
      master:
        state: active
        uri: storage@127.0.0.1:3301
        uuid: <storage_1_a>
  bucket:
    receiving: 0
    active: 2
    total: 2
    garbage: 0
    pinned: 0
    sending: 0
  status: 0
  replication:
    status: master
  alerts: []
...
_ = test_run:cmd("stop server storage_2_b")
---
...
vshard.storage.info()
---
- replicasets:
    <replicaset_2>:
      uuid: <replicaset_2>
      master:
        state: active
        uri: storage@127.0.0.1:3303
        uuid: <storage_2_a>
    <replicaset_1>:
      uuid: <replicaset_1>
      master:
        state: active
        uri: storage@127.0.0.1:3301
        uuid: <storage_1_a>
  bucket:
    receiving: 0
    active: 2
    total: 2
    garbage: 0
    pinned: 0
    sending: 0
  status: 3
  replication:
    status: master
  alerts:
  - ['UNREACHABLE_REPLICA', 'Replica <storage_2_b> isn''t active']
  - ['UNREACHABLE_REPLICASET', 'There is no active replicas in replicaset <replicaset_2>']
...
_ = test_run:cmd("start server storage_2_b")
---
...
_ = test_run:switch("storage_2_b")
---
...
vshard.storage.info()
---
- replicasets:
    <replicaset_2>:
      uuid: <replicaset_2>
      master:
        uri: storage@127.0.0.1:3303
    <replicaset_1>:
      uuid: <replicaset_1>
      master:
        uri: storage@127.0.0.1:3301
  bucket:
    receiving: 0
    active: 2
    total: 2
    garbage: 0
    pinned: 0
    sending: 0
  status: 0
  replication:
    status: follow
    lag: <lag>
  alerts: []
...
_ = test_run:switch("storage_2_a")
---
...
vshard.storage.info()
---
- replicasets:
    <replicaset_2>:
      uuid: <replicaset_2>
      master:
        state: active
        uri: storage@127.0.0.1:3303
        uuid: <storage_2_a>
    <replicaset_1>:
      uuid: <replicaset_1>
      master:
        state: active
        uri: storage@127.0.0.1:3301
        uuid: <storage_1_a>
  bucket:
    receiving: 0
    active: 2
    total: 2
    garbage: 0
    pinned: 0
    sending: 0
  status: 0
  replication:
    status: master
  alerts: []
...
_ = test_run:switch("storage_1_a")
---
...
_ = test_run:cmd("setopt delimiter ';'")
---
...
box.begin()
for id = 1, 8 do
    local bucket_id = id % 4
    box.space.test:insert({id, bucket_id})
    for id2 = id * 10, id * 10 + 2 do
        box.space.test2:insert({id2, id, bucket_id})
    end
end
box.commit();
---
...
_ = test_run:cmd("setopt delimiter ''");
---
...
box.space.test:select()
---
- - [1, 1]
  - [2, 2]
  - [3, 3]
  - [4, 0]
  - [5, 1]
  - [6, 2]
  - [7, 3]
  - [8, 0]
...
box.space.test2:select()
---
- - [10, 1, 1]
  - [11, 1, 1]
  - [12, 1, 1]
  - [20, 2, 2]
  - [21, 2, 2]
  - [22, 2, 2]
  - [30, 3, 3]
  - [31, 3, 3]
  - [32, 3, 3]
  - [40, 4, 0]
  - [41, 4, 0]
  - [42, 4, 0]
  - [50, 5, 1]
  - [51, 5, 1]
  - [52, 5, 1]
  - [60, 6, 2]
  - [61, 6, 2]
  - [62, 6, 2]
  - [70, 7, 3]
  - [71, 7, 3]
  - [72, 7, 3]
  - [80, 8, 0]
  - [81, 8, 0]
  - [82, 8, 0]
...
vshard.storage.bucket_collect(1)
---
- - - test2
    - - [10, 1, 1]
      - [11, 1, 1]
      - [12, 1, 1]
  - - test
    - - [1, 1]
      - [5, 1]
...
vshard.storage.bucket_collect(2)
---
- - - test2
    - - [20, 2, 2]
      - [21, 2, 2]
      - [22, 2, 2]
  - - test
    - - [2, 2]
      - [6, 2]
...
box.space.test:get{1}
---
- [1, 1]
...
vshard.storage.call(1, 'read', 'space_get', {'test', {1}})
---
- true
- [1, 1]
...
vshard.storage.call(100500, 'read', 'space_get', {'test', {1}})
---
- null
- bucket_id: 100500
  reason: Not found
  code: 1
  type: ShardingError
  message: 'Cannot perform action with bucket 100500, reason: Not found'
  name: WRONG_BUCKET
...
--
-- Test not existing uuid.
--
vshard.storage.bucket_recv(100, 'from_uuid', {{1000, {{1}}}})
---
- null
- code: 4
  type: ShardingError
  name: NO_SUCH_REPLICASET
  replicaset_uuid: from_uuid
  message: Replicaset from_uuid not found
...
--
-- Test not existing space in bucket data.
--
res, err = vshard.storage.bucket_recv(4, util.replicasets[2], {{1000, {{1}}}})
---
...
util.portable_error(err)
---
- type: ClientError
  message: Space '1000' does not exist
...
while box.space._bucket:get{4} do vshard.storage.recovery_wakeup() fiber.sleep(0.01) end
---
...
--
-- gh-275: detailed info when couldn't insert into a space.
--
res, err = vshard.storage.bucket_recv(                                          \
    4, util.replicasets[2], {{box.space.test.id, {{9, 4}, {10, 4}, {1, 4}}}})
---
...
assert(not res)
---
- true
...
assert(err.space == 'test')
---
- true
...
assert(err.bucket_id == 4)
---
- true
...
assert(tostring(err.tuple) == '[1, 4]')
---
- true
...
assert(err.reason:match('Duplicate key exists') ~= nil)
---
- true
...
err = err.message
---
...
assert(err:match('bucket 4 data in space "test" at tuple %[1, 4%]') ~= nil)
---
- true
...
assert(err:match('Duplicate key exists') ~= nil)
---
- true
...
while box.space._bucket:get{4} do                                               \
    vshard.storage.recovery_wakeup() fiber.sleep(0.01)                          \
end
---
...
assert(box.space.test:get{9} == nil and box.space.test:get{10} == nil)
---
- true
...
--
-- Bucket transfer
--
-- Transfer to unknown replicaset.
vshard.storage.bucket_send(1, 'unknown uuid')
---
- null
- code: 4
  type: ShardingError
  name: NO_SUCH_REPLICASET
  replicaset_uuid: unknown uuid
  message: Replicaset unknown uuid not found
...
-- gh-217: transfer to self.
vshard.storage.bucket_send(1, util.replicasets[1])
---
- null
- bucket_id: 1
  code: 5
  type: ShardingError
  name: MOVE_TO_SELF
  replicaset_uuid: <replicaset_1>
  message: 'Cannot move: bucket 1 is already on replicaset <replicaset_1>'
...
-- Successful transfer.
vshard.storage.bucket_send(1, util.replicasets[2])
---
- true
...
wait_bucket_is_collected(1)
---
...
_ = test_run:switch("storage_2_a")
---
...
vshard.storage.buckets_info()
---
- 1:
    status: active
    id: 1
  3:
    status: active
    id: 3
  4:
    status: active
    id: 4
...
_ = test_run:switch("storage_1_a")
---
...
vshard.storage.buckets_info()
---
- 2:
    status: active
    id: 2
...
--
-- Part of gh-76: check that netbox old connections are reused on
-- reconfiguration.
--
old_connections = {}
---
...
connection_count = 0
---
...
old_replicasets = vshard.storage.internal.replicasets
---
...
_ = test_run:cmd("setopt delimiter ';'")
---
...
for _, old_replicaset in pairs(old_replicasets) do
	for uuid, old_replica in pairs(old_replicaset.replicas) do
		old_connections[uuid] = old_replica.conn
		if old_replica.conn then
			connection_count = connection_count + 1
		end
	end
end;
---
...
_ = test_run:cmd("setopt delimiter ''");
---
...
connection_count > 0
---
- true
...
vshard.storage.cfg(cfg, util.name_to_uuid.storage_1_a)
---
...
new_replicasets = vshard.storage.internal.replicasets
---
...
new_replicasets ~= old_replicasets
---
- true
...
_ = test_run:cmd("setopt delimiter ';'")
---
...
for _, new_replicaset in pairs(new_replicasets) do
	for uuid, new_replica in pairs(new_replicaset.replicas) do
		assert(old_connections[uuid] == new_replica.conn)
	end
end;
---
...
_ = test_run:cmd("setopt delimiter ''");
---
...
-- gh-114: Check non-dynamic option change during reconfigure.
non_dynamic_cfg = table.copy(cfg)
---
...
non_dynamic_cfg.bucket_count = require('vshard.consts').DEFAULT_BUCKET_COUNT + 1
---
...
util.check_error(vshard.storage.cfg, non_dynamic_cfg, util.name_to_uuid.storage_1_a)
---
- Non-dynamic option bucket_count cannot be reconfigured
...
-- Error during reconfigure process.
_, rs = next(vshard.storage.internal.replicasets)
---
...
rs:callro('echo', {'some_data'})
---
- some_data
- null
- null
...
vshard.storage.internal.errinj.ERRINJ_CFG = true
---
...
old_internal = table.copy(vshard.storage.internal)
---
...
util.check_error(vshard.storage.cfg, cfg, util.name_to_uuid.storage_1_a)
---
- 'Error injection: cfg'
...
vshard.storage.internal.errinj.ERRINJ_CFG = false
---
...
util.has_same_fields(old_internal, vshard.storage.internal)
---
- true
...
_, rs = next(vshard.storage.internal.replicasets)
---
...
rs:callro('echo', {'some_data'})
---
- some_data
- null
- null
...
--
-- Bucket count is calculated properly.
--
-- Cleanup after the previous tests.
_ = test_run:switch('default')
---
...
util.map_bucket_protection(test_run, {REPLICASET_1, REPLICASET_2}, false)
---
...
_ = test_run:switch('storage_1_a')
---
...
buckets = vshard.storage.buckets_info()
---
...
for bid, _ in pairs(buckets) do vshard.storage.bucket_force_drop(bid) end
---
...
vshard.storage.sync()
---
- true
...
_ = test_run:switch('storage_2_a')
---
...
buckets = vshard.storage.buckets_info()
---
...
for bid, _ in pairs(buckets) do vshard.storage.bucket_force_drop(bid) end
---
...
vshard.storage.sync()
---
- true
...
_ = test_run:switch('default')
---
...
util.map_bucket_protection(test_run, {REPLICASET_1, REPLICASET_2}, true)
---
...
_ = test_run:switch('storage_1_a')
---
...
assert(vshard.storage.buckets_count() == 0)
---
- true
...
test_run:wait_lsn('storage_1_b', 'storage_1_a')
---
...
_ = test_run:switch('storage_1_b')
---
...
--
-- gh-276: bucket count cache should be properly updated on the replica nodes.
-- For that the replicas must also install on_replace trigger on _bucket space
-- to watch for changes.
--
assert(vshard.storage.buckets_count() == 0)
---
- true
...
_ = test_run:switch('storage_1_a')
---
...
vshard.storage.bucket_force_create(1, 5)
---
- true
...
assert(vshard.storage.buckets_count() == 5)
---
- true
...
test_run:wait_lsn('storage_1_b', 'storage_1_a')
---
...
_ = test_run:switch('storage_1_b')
---
...
assert(vshard.storage.buckets_count() == 5)
---
- true
...
_ = test_run:switch('storage_1_a')
---
...
vshard.storage.bucket_force_create(6, 5)
---
- true
...
assert(vshard.storage.buckets_count() == 10)
---
- true
...
test_run:wait_lsn('storage_1_b', 'storage_1_a')
---
...
_ = test_run:switch('storage_1_b')
---
...
assert(vshard.storage.buckets_count() == 10)
---
- true
...
--
-- Bucket_generation_wait() registry function.
--
_ = test_run:switch('storage_1_a')
---
...
lstorage = require('vshard.registry').storage
---
...
ok, err = lstorage.bucket_generation_wait(-1)
---
...
assert(not ok and err.message)
---
- Timeout exceeded
...
ok, err = lstorage.bucket_generation_wait(0)
---
...
assert(not ok and err.message)
---
- Timeout exceeded
...
small_timeout = 0.000001
---
...
ok, err = lstorage.bucket_generation_wait(small_timeout)
---
...
assert(not ok and err.message)
---
- Timeout exceeded
...
ok, err = nil
---
...
big_timeout = 1000000
---
...
_ = fiber.create(function()                                                     \
    ok, err = lstorage.bucket_generation_wait(big_timeout)                      \
end)
---
...
fiber.sleep(small_timeout)
---
...
assert(not ok and not err)
---
- true
...
_ = test_run:switch('default')
---
...
util.map_bucket_protection(test_run, {REPLICASET_1}, false)
---
...
_ = test_run:switch('storage_1_a')
---
...
vshard.storage.bucket_force_drop(10)
---
- true
...
vshard.storage.sync()
---
- true
...
_ = test_run:switch('default')
---
...
util.map_bucket_protection(test_run, {REPLICASET_1}, true)
---
...
_ = test_run:switch('storage_1_a')
---
...
test_run:wait_cond(function() return ok or err end)
---
- true
...
assert(ok)
---
- true
...
-- Cancel should interrupt the waiting.
ok, err = nil
---
...
f = fiber.create(function()                                                     \
    ok, err = lstorage.bucket_generation_wait(big_timeout)                      \
end)
---
...
fiber.sleep(small_timeout)
---
...
assert(not ok and not err)
---
- true
...
f:cancel()
---
...
_ = test_run:wait_cond(function() return ok or err end)
---
...
assert(not ok and err.message)
---
- fiber is cancelled
...
--
-- Bucket_are_all_rw() registry function.
--
assert(lstorage.bucket_are_all_rw())
---
- true
...
vshard.storage.internal.errinj.ERRINJ_RECOVERY_PAUSE = true
---
...
-- Let it stuck in the errinj.
vshard.storage.recovery_wakeup()
---
...
vshard.storage.bucket_force_create(10)
---
- true
...
box.space._bucket:update(10, {{'=', 2, vshard.consts.BUCKET.SENDING}})
---
- [10, 'sending']
...
assert(not lstorage.bucket_are_all_rw())
---
- true
...
box.space._bucket:update(10, {{'=', 2, vshard.consts.BUCKET.ACTIVE}})
---
- [10, 'active']
...
assert(lstorage.bucket_are_all_rw())
---
- true
...
vshard.storage.internal.errinj.ERRINJ_RECOVERY_PAUSE = false
---
...
--
-- Internal info function.
--
vshard.storage._call('info')
---
- is_master: true
...
_ = test_run:switch('storage_1_b')
---
...
vshard.storage._call('info')
---
- is_master: false
...
--
-- gh-123, gh-298: storage auto-enable/disable depending on instance state.
--
_ = test_run:switch('default')
---
...
_ = test_run:cmd('stop server storage_1_a')
---
...
_ = test_run:cmd('start server storage_1_a with args="boot_before_cfg"')
---
...
-- We should wait for upstream on storage_1_b, as overwise we can get
-- IPROTO_VOTE from it, which tries to access box.cfg.read_only for ballot
-- generation. It won't be able to do that as at this point box.cfg will be
-- a function, and tarantool will fail with panic.
util.wait_master(test_run, REPLICASET_1, 'storage_1_a')
---
...
_ = test_run:switch('storage_1_a')
---
...
-- Leaving box.cfg() not called won't work because at 1.10 test-run somewhy
-- raises an error when try to start an instance without box.cfg(). It can only
-- be emulated.
old_cfg = box.cfg
---
...
assert(type(old_cfg) == 'table')
---
- true
...
box.cfg = function(...) return old_cfg(...) end
---
...
ok, err = pcall(vshard.storage.call, 1, 'read', 'echo', {100})
---
...
assert(not ok and err.code == vshard.error.code.STORAGE_IS_DISABLED)
---
- true
...
assert(err.message:match('box seems not to be configured') ~= nil)
---
- true
...
box.cfg = old_cfg
---
...
-- Disabled until box is loaded.
vshard.storage.internal.errinj.ERRINJ_CFG_DELAY = true
---
...
f = fiber.create(vshard.storage.cfg, cfg, instance_uuid)
---
...
f:set_joinable(true)
---
...
old_info = box.info
---
...
box.info = {status = 'loading'}
---
...
ok, err = pcall(vshard.storage.call, 1, 'read', 'echo', {100})
---
...
assert(not ok and err.code == vshard.error.code.STORAGE_IS_DISABLED)
---
- true
...
assert(err.message:match('instance status is "loading"') ~= nil)
---
- true
...
box.info = old_info
---
...
-- Disabled until storage is configured.
test_run:wait_cond(function() return box.info.status == 'running' end)
---
- true
...
ok, err = pcall(vshard.storage.call, 1, 'read', 'echo', {100})
---
...
assert(not ok and err.code == vshard.error.code.STORAGE_IS_DISABLED)
---
- true
...
assert(err.message:match('storage is not configured') ~= nil)
---
- true
...
vshard.storage.internal.errinj.ERRINJ_CFG_DELAY = false
---
...
f:join()
---
- true
...
-- Enabled when all criteria are finally satisfied.
ok, res = vshard.storage.call(1, 'read', 'echo', {100})
---
...
assert(ok and res == 100)
---
- true
...
--
-- Manual enable/disable.
--
vshard.storage.disable()
---
...
ok, err = pcall(vshard.storage.call, 1, 'read', 'echo', {100})
---
...
assert(not ok and err.code == vshard.error.code.STORAGE_IS_DISABLED)
---
- true
...
assert(err.message:match('storage is disabled explicitly') ~= nil)
---
- true
...
vshard.storage.enable()
---
...
ok, res = vshard.storage.call(1, 'read', 'echo', {100})
---
...
assert(ok and res == 100)
---
- true
...
_ = test_run:switch("default")
---
...
test_run:drop_cluster(REPLICASET_2)
---
...
test_run:drop_cluster(REPLICASET_1)
---
...
_ = test_run:cmd('clear filter')
---
...
